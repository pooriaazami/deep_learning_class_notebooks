{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMzEZjTDTJxfI3u0MIefDO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pooriaazami/deep_learning_class_notebooks/blob/main/13_Machine_Translation_From_Scratch_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cezeGfesoylg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "RCpeVTNlvgxg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, num_tokens, embedding_dim, latent_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(num_embeddings=num_tokens, embedding_dim=embedding_dim)\n",
        "    self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=latent_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    batch_size, _, _ = x.size()\n",
        "    h_0 = torch.zeros(2, batch_size, self.latent_dim).to(DEVICE)\n",
        "    outputs, context_vector = self.rnn(x, h_0)\n",
        "\n",
        "    return context_vector, outputs"
      ],
      "metadata": {
        "id": "l6drzN3TvYEP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(num_tokens=100, embedding_dim=16, latent_dim=64).to(DEVICE)"
      ],
      "metadata": {
        "id": "uEfD71WEvcs0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "seq_length = 30\n",
        "\n",
        "test_input = torch.zeros(batch_size, seq_length, dtype=torch.int64)\n",
        "test_output, _ = encoder(test_input)\n",
        "\n",
        "test_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKKSzpd_voel",
        "outputId": "ed6adfc9-9c52-4b02-a511-8fff8f3988de"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "  def __init__(self, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "    self.U = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "    self.V = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "  def forward(self, query, keys):\n",
        "    QK = self.W(query).unsqueeze(1) + self.U(keys)\n",
        "    QK = torch.tanh(QK)\n",
        "    scores = self.V(QK)\n",
        "\n",
        "    scores = scores.squeeze(2).unsqueeze(1)\n",
        "    weigths = F.softmax(scores, dim=-1)\n",
        "\n",
        "    context = torch.bmm(weigths, keys)\n",
        "\n",
        "    return context, weigths"
      ],
      "metadata": {
        "id": "QoAL81lzwBtT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionGRU(nn.Module):\n",
        "  def __init__(self, input_size, latent_dim):\n",
        "    super().__init__()\n",
        "    self.attention = AttentionBlock(2 * latent_dim)\n",
        "    self.rnn = nn.GRU(input_size=input_size, hidden_size=2 * latent_dim)\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "  def forward(self, predicted_label, encoder_outputs):\n",
        "    batch_size, _, _ = predicted_label.size()\n",
        "    h = torch.zeros(batch_size, 2 * self.latent_dim)\n",
        "    predicted_label = predicted_label.permute(1, 0, 2)\n",
        "    for token in predicted_label:\n",
        "      context, weights = self.attention(h, encoder_outputs)\n",
        "      context = context.permute(1, 0, 2)\n",
        "      token = token.unsqueeze(1).permute(1, 0, 2)\n",
        "      output, h = self.rnn(token, context)\n",
        "      h = h.squeeze()\n",
        "\n",
        "    return output, h"
      ],
      "metadata": {
        "id": "FHt-pZmz40N1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, num_tokens, embedding_dim, latent_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(num_embeddings=num_tokens, embedding_dim=embedding_dim)\n",
        "    self.rnn = AttentionGRU(embedding_dim, latent_dim)\n",
        "    self.fc = nn.Linear(in_features=2 * latent_dim, out_features=num_tokens)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, encoder_outputs, predicted_label):\n",
        "    x = self.embedding(predicted_label)\n",
        "    x, _ = self.rnn(x, encoder_outputs)\n",
        "    x = self.fc(x)\n",
        "    x = self.softmax(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "nfrRgkJF62bE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(num_tokens=100, embedding_dim=8, latent_dim=16)\n",
        "decoder = Decoder(num_tokens=100, embedding_dim=8, latent_dim=16)"
      ],
      "metadata": {
        "id": "lrVicVYv7LBr"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "seq_length = 20\n",
        "predicted_labels_count = 10\n",
        "test_input = torch.zeros(batch_size, seq_length, dtype=torch.int64)\n",
        "predicted_labels = torch.zeros(batch_size, predicted_labels_count, dtype=torch.int64)\n",
        "\n",
        "_, encoder_output = encoder(test_input)\n",
        "new_token = decoder(encoder_output, predicted_labels)\n",
        "new_token.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-glyCsm7T29",
        "outputId": "4e784ab3-7cef-4924-ecb7-3d0ee3b7b8e5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 50, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ncvEzVsD79Zh"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}